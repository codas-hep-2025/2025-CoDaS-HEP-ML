{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":108149,"databundleVersionId":13104605,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Paths\nbase_path = '/kaggle/input/qcd-tt-jet-tagging-co-da-s-hep'\n\n# Load training data\ntrain_df = pd.read_csv(f'{base_path}/train/features/cluster_features.csv')\ntrain_labels = np.load(f'{base_path}/train/labels/labels.npy')\ntrain_ids = np.load(f'{base_path}/train/ids/ids.npy')\n\n# Load validation data\nval_df = pd.read_csv(f'{base_path}/val/features/cluster_features.csv')\nval_labels = np.load(f'{base_path}/val/labels/labels.npy')\nval_ids = np.load(f'{base_path}/val/ids/ids.npy')\n\n# Combine train + val\nX = pd.concat([train_df, val_df], axis=0).reset_index(drop=True)\ny = np.concatenate([train_labels, val_labels])\nids = np.concatenate([train_ids, val_ids])\n\nX['pt_per_cluster'] = X['total_pt'] / (X['n_clusters'] + 1e-6)\nX['cluster_size_diff'] = X['max_cluster_size'] - X['mean_cluster_size']\nX['pt_std_ratio'] = X['std_cluster_pt'] / (X['mean_cluster_pt'] + 1e-6)\nX['pt_ratio'] = X['max_cluster_pt'] / (X['total_pt'] + 1e-6)\nX['cluster_density'] = X['n_clusters'] / (X['max_cluster_size'] + 1e-6)\nX['inv_mean_size'] = 1 / (X['mean_cluster_size'] + 1e-6)\nX['pt_entropy'] = -X['std_cluster_pt'] * np.log(X['mean_cluster_pt'] + 1e-6)\n\n\n# Cross-validation training\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ntest_df = pd.read_csv(f'{base_path}/test/features/cluster_features.csv')\ntest_ids = np.load(f'{base_path}/test/ids/ids.npy')\n\n# Apply same features to test set\ntest_df['pt_per_cluster'] = test_df['total_pt'] / (test_df['n_clusters'] + 1e-6)\ntest_df['cluster_size_diff'] = test_df['max_cluster_size'] - test_df['mean_cluster_size']\ntest_df['pt_std_ratio'] = test_df['std_cluster_pt'] / (test_df['mean_cluster_pt'] + 1e-6)\ntest_df['pt_ratio'] = test_df['max_cluster_pt'] / (test_df['total_pt'] + 1e-6)\ntest_df['cluster_density'] = test_df['n_clusters'] / (test_df['max_cluster_size'] + 1e-6)\ntest_df['inv_mean_size'] = 1 / (test_df['mean_cluster_size'] + 1e-6)\ntest_df['pt_entropy'] = -test_df['std_cluster_pt'] * np.log(test_df['mean_cluster_pt'] + 1e-6)\n\n\n\ntest_preds_all = np.zeros(len(test_df))\nval_preds_all = []\nval_targets_all = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    model = XGBClassifier(\n        n_estimators=200,\n        max_depth=6,\n        learning_rate=0.05,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        use_label_encoder=False,\n        eval_metric='logloss',\n        random_state=fold\n    )\n\n    model.fit(X_train, y_train)\n    val_preds = model.predict_proba(X_val)[:, 1]\n    val_preds_all.extend(val_preds)\n    val_targets_all.extend(y_val)\n\n    test_preds_all += model.predict_proba(test_df)[:, 1] / skf.n_splits\n\n# Evaluate\ncv_auc = roc_auc_score(val_targets_all, val_preds_all)\nprint(f'Cross-Validated AUC: {cv_auc:.4f}')\nxgb_preds = test_preds_all.copy()\nnp.save('xgb_preds.npy', xgb_preds)\n\n# Plot confidence distribution\nplt.hist(test_preds_all, bins=50)\nplt.title(\"Test Set Prediction Distribution\")\nplt.xlabel(\"Probability of ttÌ„\")\nplt.ylabel(\"Count\")\nplt.grid(True)\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T06:40:28.683065Z","iopub.execute_input":"2025-07-24T06:40:28.683406Z","iopub.status.idle":"2025-07-24T06:40:30.526794Z","shell.execute_reply.started":"2025-07-24T06:40:28.683383Z","shell.execute_reply":"2025-07-24T06:40:30.525896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX = pd.concat([\n    pd.read_csv(f'{base_path}/train/features/cluster_features.csv'),\n    pd.read_csv(f'{base_path}/val/features/cluster_features.csv')\n]).reset_index(drop=True)\n\ny = np.concatenate([\n    np.load(f'{base_path}/train/labels/labels.npy'),\n    np.load(f'{base_path}/val/labels/labels.npy')\n])\n\n# Same feature engineering\nX['pt_per_cluster'] = X['total_pt'] / (X['n_clusters'] + 1e-6)\nX['cluster_size_diff'] = X['max_cluster_size'] - X['mean_cluster_size']\nX['pt_std_ratio'] = X['std_cluster_pt'] / (X['mean_cluster_pt'] + 1e-6)\nX['pt_ratio'] = X['max_cluster_pt'] / (X['total_pt'] + 1e-6)\nX['cluster_density'] = X['n_clusters'] / (X['max_cluster_size'] + 1e-6)\nX['inv_mean_size'] = 1 / (X['mean_cluster_size'] + 1e-6)\nX['pt_entropy'] = -X['std_cluster_pt'] * np.log(X['mean_cluster_pt'] + 1e-6)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train-validation split for DNN\nX_train, X_val, y_train, y_val = train_test_split(\n    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Build the DNN model\nmodel_dnn = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel_dnn.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['AUC']\n)\n\nmodel_dnn.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=30,\n    batch_size=64,\n    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n    verbose=2\n)\n\n\nX_test = pd.read_csv(f'{base_path}/test/features/cluster_features.csv')\n\n# Apply the same feature engineering\nX_test['pt_per_cluster'] = X_test['total_pt'] / (X_test['n_clusters'] + 1e-6)\nX_test['cluster_size_diff'] = X_test['max_cluster_size'] - X_test['mean_cluster_size']\nX_test['pt_std_ratio'] = X_test['std_cluster_pt'] / (X_test['mean_cluster_pt'] + 1e-6)\nX_test['pt_ratio'] = X_test['max_cluster_pt'] / (X_test['total_pt'] + 1e-6)\nX_test['cluster_density'] = X_test['n_clusters'] / (X_test['max_cluster_size'] + 1e-6)\nX_test['inv_mean_size'] = 1 / (X_test['mean_cluster_size'] + 1e-6)\nX_test['pt_entropy'] = -X_test['std_cluster_pt'] * np.log(X_test['mean_cluster_pt'] + 1e-6)\n\nX_test_scaled = scaler.transform(X_test)\n\n# Predict test probabilities\ndnn_preds = model_dnn.predict(X_test_scaled).flatten()\nnp.save('dnn_preds.npy', dnn_preds)\nprint(\"dnn_preds generated and saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T06:41:32.781306Z","iopub.execute_input":"2025-07-24T06:41:32.781743Z","iopub.status.idle":"2025-07-24T06:41:46.160886Z","shell.execute_reply.started":"2025-07-24T06:41:32.781718Z","shell.execute_reply":"2025-07-24T06:41:46.159942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import h5py\n\n\n\n# Load jet image data (train + val)\nwith h5py.File(f'{base_path}/train/images/jet_images.h5', 'r') as f: \n    train_images = f['images'][:]  # shape: (N, 32, 32)\n\nwith h5py.File(f'{base_path}/val/images/jet_images.h5', 'r') as f:\n    val_images = f['images'][:]\n\nimages = np.concatenate([train_images, val_images], axis=0)\nlabels = np.concatenate([\n    np.load(f'{base_path}/train/labels/labels.npy'),\n    np.load(f'{base_path}/val/labels/labels.npy')\n])\n\n# Normalize and reshape\nimages = images.astype('float32') / np.max(images)\nimages = np.expand_dims(images, axis=-1)  # shape becomes (N, 32, 32, 1)\n\n# Split for validation\nX_train, X_val, y_train, y_val = train_test_split(\n    images, labels, test_size=0.2, stratify=labels, random_state=42\n)\n\n# Build CNN model\nmodel_cnn = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(32, 32, 1)),\n    tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=2),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=2),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n\n# Train the CNN\nmodel_cnn.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=20,\n    batch_size=64,\n    callbacks=[tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)],\n    verbose=2\n)\n\n# Load test images\nwith h5py.File(f'{base_path}/test/images/jet_images.h5', 'r') as f:\n    test_images = f['images'][:]\ntest_images = test_images.astype('float32') / np.max(test_images)\ntest_images = np.expand_dims(test_images, axis=-1)\n\n# Predict\ncnn_preds = model_cnn.predict(test_images).flatten()\nnp.save('cnn_preds.npy', cnn_preds)\nprint(\"cnn_preds generated and saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T06:51:30.855937Z","iopub.execute_input":"2025-07-24T06:51:30.856274Z","iopub.status.idle":"2025-07-24T06:51:48.781213Z","shell.execute_reply.started":"2025-07-24T06:51:30.856250Z","shell.execute_reply":"2025-07-24T06:51:48.780214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Load predictions\nxgb_preds = np.load('xgb_preds.npy')\ndnn_preds = np.load('dnn_preds.npy')\ncnn_preds = np.load('cnn_preds.npy')\ntest_ids = np.load('/kaggle/input/qcd-tt-jet-tagging-co-da-s-hep/test/ids/ids.npy')\n\n\nweights = np.array([0.2, 0.45, 0.35]) \n\n# Normalize just in case\nweights = weights / np.sum(weights)\n\n# Ensemble\nfinal_preds = weights[0] * xgb_preds + weights[1] * dnn_preds + weights[2] * cnn_preds\n\n# Save submission\nsubmission = pd.DataFrame({'id': test_ids, 'label': final_preds})\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(f\"Submission saved with weights: XGB={weights[0]:.2f}, DNN={weights[1]:.2f}, CNN={weights[2]:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T06:55:06.134469Z","iopub.execute_input":"2025-07-24T06:55:06.135144Z","iopub.status.idle":"2025-07-24T06:55:06.149595Z","shell.execute_reply.started":"2025-07-24T06:55:06.135112Z","shell.execute_reply":"2025-07-24T06:55:06.148751Z"}},"outputs":[],"execution_count":null}]}